In the wake of Donald Trump's unexpected victory, many questions have beenraised about Facebook's role in the promotion of inaccurate and highly partisaninformation during the presidential race and whether this fake news influencedthe election's outcome.A few have downplayed Facebook's impact, including CEO Mark Zuckerberg, who saidthat it is "extremely unlikely" that fake news could have swayed the election.But questions about the social network's political significance merit more thanpassing attention.Do Facebook's filtering algorithms explain why so many liberals had misplacedconfidence in a Clinton victory (echoing the error made by Romney supporters in2012)? And is the fake news being circulated on Facebook the reason that so manyTrump supporters have endorsed demonstrably false statements made by theircandidate?The popular claim that "filter bubbles" are why fake news thrives on Facebook isalmost certainly wrong. If the network is encouraging people to believe untruths- and that's a big if - the problem more likely lies in how the platforminteracts with basic human social tendencies. That's far more difficult tochange.A misinformed publicFacebook's role in the dissemination of political news is undeniable. In May2016, 44 percent of Americans said they got news from the social media site. Andthe prevalence of misinformation disseminated through Facebook is undeniable.It's plausible, then, that the amount of fake news on a platform where so manypeople get their news can help explain why so many Americans are misinformedabout politics.But it's hard to say how likely this is. I began studying the internet's role inpromoting false beliefs during the 2008 election, turning my attention to socialmedia in 2012. In ongoing research, I've found little consistent evidence thatsocial media use promoted acceptance of false claims about the candidates,despite the prevalence of many untruths. Instead, it appears that in 2012, as in2008, email continued to be a uniquely powerful conduit for lies and conspiracytheories. Social media had no reliably detectable effect on people's beliefs.For a moment, however, let's suppose that 2016 was different from 2012 and 2008.(The election was certainly unique in many other regards.)If Facebook is promoting a platform in which citizens are less able to discerntruth from fiction, it would constitute a serious threat to American democracy.But naming the problem isn't enough. To fight the flow of misinformation throughsocial media, it's important to understand why it happens.Don't blame filter bubblesFacebook wants its users to be engaged, not overwhelmed, so it employsproprietary software that filters users' news feeds and chooses the content thatwill appear. The risk lies in how this tailoring is done.There's ample evidence that people are drawn to news that affirms theirpolitical viewpoint. Facebook's software learns from users' past actions; ittries to guess which stories they are likely to click or share in the future.Taken to its extreme, this produces a filter bubble, in which users are exposedonly to content that reaffirms their biases. The risk, then, is that filterbubbles promote misperceptions by hiding the truth.The appeal of this explanation is obvious. It's easy to understand, so maybeit'll be easy to fix. Get rid of personalized news feeds, and filter bubbles areno more.The problem with the filter bubble metaphor is that it assumes people areperfectly insulated from other perspectives. In fact, numerousstudieshaveshownthat individuals' media diets almost always include information and sources thatchallenge their political attitudes. And a study of Facebook user data foundthat encounters with cross-cutting information is widespread. In other words,holding false beliefs is unlikely to be explained by people's lack of contactwith more accurate news.Instead, people's preexisting political identities profoundly shape theirbeliefs. So even when faced with the same information, whether it's a newsarticle or a fact check, people with different political orientations oftenextract dramatically different meaning.A thought experiment may help: If you were a Clinton supporter, were you awarethat the highly respected prediction site FiveThirtyEight gave Clinton only a 71percent chance of winning? Those odds are better than a coin flip, but far froma sure thing. I suspect that many Democrats were shocked despite seeing thisuncomfortable evidence. Indeed, many had been critical of this projection in thedays before the election.If you voted for Trump, have you ever encountered evidence disputing Trump'sassertion that voter fraud is commonplace in the U.S.? Fact checkers and newsorganizations have covered this issue extensively, offering robust evidence thatthe claim is untrue. However a Trump supporter might be unmoved: In a September2016 poll, 90 percent of Trump supporters said they didn't trust fact checkers.Facebook = angry partisans?If isolation from the truth really is the main source of inaccurate information,the solution would be obvious: Make the truth more visible.Unfortunately, the answer isn't that simple. Which brings us back to thequestion of Facebook: Are there other aspects of the service that might distortusers' beliefs?It will be some time before researchers can answer this question confidently,but as someone who has studied how the various ways that other internettechnologies can lead people to believe false information, I'm prepared to offera few educated guesses.There are two things that we already know about Facebook that could encouragethe spread of false information.First, emotions are contagious, and they can spread on Facebook. One large-scalestudy has shown that small changes in Facebook users' news feeds can shape theemotions they express in later posts. In that study, the emotional changes weresmall, but so were the changes in the news feed that caused them. Just imaginehow Facebook users respond to widespread accusations of candidates' corruption,criminal activity and lies. It isn't surprising that nearly half (49 percent) ofall users described political discussion on social media as "angry."When it comes to politics, anger is a powerful emotion. It's been shown to makepeople more willing to accept partisan falsehoods and more likely to post andshare political information, presumably including fake news articles thatreinforce their beliefs. If Facebook use makes partisans angry while alsoexposing them to partisan falsehoods, ensuring the presence of accurateinformation may not matter much. Republican or Democrat, angry people put theirtrust in information that makes their side look good.Second, Facebook seems to reinforce people's political identity - furthering analready large partisan divide. While Facebook doesn't shield people frominformation they disagree with, it certainly makes it easier to find like-mindedothers. Our social networks tend to include many people who share our values andbeliefs. And this may be another way that Facebook is reinforcing politicallymotivated falsehoods. Beliefs often serve a social function, helping people todefine who they are and how they fit in the world. The easier it is for peopleto see themselves in political terms, the more attached they are to the beliefsthat affirm that identity.These two factors - the way that anger can spread over Facebook's socialnetworks, and how those networks can make individuals' political identity morecentral to who they are - likely explain Facebook users' inaccurate beliefs moreeffectively than the so-called filter bubble.If this is true, then we have a serious challenge ahead of us. Facebook willlikely be convinced to change its filtering algorithm to prioritize moreaccurate information. Google has already undertaken a similar endeavor. Andrecent reports suggest that Facebook may be taking the problem more seriouslythan Zuckerberg's comments suggest.But this does nothing to address the underlying forces that propagate andreinforce false information: emotions and the people in your social networks.Nor is it obvious that these characteristics of Facebook can or should be"corrected." A social network devoid of emotion seems like a contradiction, andpolicing who individuals interact with is not something that our society shouldembrace.It may be that Facebook shares some of the blame for some of the lies thatcirculated this election year - and that they altered the course of theelection.If true, the challenge will be to figure out what we can do about it.R. Kelly Garrett is an Associate Professor of Communication at The Ohio StateUniversity. This article was originally published on The Conversation. Read theoriginal article.