Political strategists                             and Patrick Ruffini onrecovering from the polling disasterPlanes rarely crash because one instrument fails or one gauge gives a badreading. Rather, the right combination of things fail in tandem - a mechanicalproblem paired with bad weather, a backup system malfunctioning at the same timeas a pilot error -  leading to catastrophe. The disaster of the 2016 electionforecasts is not dissimilar, with a series of mistakes building upon one anotherto lead prognosticators astray. Pollsters now are sifting through the wreckageto find the black boxes and assess what went wrong in order to prevent it fromhappening again.The good news is that at the same moment polling feels a bit broken, there arenew tools available for measuring the mood of the people, the tides of publicopinion and the swings of an election. These tools - including precisedemographic models and social-media data - shouldn't replace polling. But theycould help refine our predictions and our understanding.Despite America's surprise at Donald Trump's victory in the presidentialelection, not all polls failed in 2016. The national polling average willprobably be fairly close to the final popular-vote result once all the remainingballots in California are counted. And polls in some states, including NewHampshire and Virginia,  got the election almost exactly right. But polls wereoff in enough other states by large enough magnitudes to mislead forecastersabout which states were in play and which way the electoral college would go.Some analysts have suggested that "shy Trump voters " were afraid to admit theirtrue preference to pollsters, but it is not clear why this would cause Trump todramatically outperform his poll numbers in some swing states but not others,and it doesn't explain why Republican Senate candidates often outperformed theirpolls by similar margins. There is some evidence for the theory that pollsters'turnout assumptions were off and that the electorate was simply more Republicanacross the board, with turnout rising the most in heavily Republican countiesand dropping the most among a core Democratic constituency: African Americans.Polls can also err because of late movement heading into Election Day,especially in an election with historically high numbers of undecided andthird-party voters. A possible clue here lies in the exit polls, which showedlate-deciding voters in those decisive Rust Belt states breaking heavily forTrump.Nate Silver of FiveThirtyEight managed to outperform his peers by assuming thatthe polls might not be as good as people thought and, as a result, believingthere were more possible paths to a Trump victory. But to always assume that thepolls might be "off" is not a very satisfying way to address the problem.Rebuilding trust in polling is key, as polls not only help politicians winelections but also offer a valuable way to tell those in "the Beltway bubble"what their constituents are thinking.In the worlds of polling and data analytics, there should be a reassessment ofhow we can accurately determine who is likely to vote. Relying on vote-historydata from voter files, rather than often-erroneous self-reports of votinghistory, is an essential element missing in most media polling. But leaning tooheavily on data from previous elections can lead us to miss what's differentabout this one. For instance, it wasn't a given that Hillary Clinton would beable to reenergize the Obama coalition. Meanwhile, conventional likely-voter"screens" - the way pollsters try to filter out nonvoters and improve therepresentation of actual voters in a sample - have failed in two election cyclesin a row, giving Republicans false hope in 2012 and doing the same for Democratsin 2016.We also may need to look beyond polls alone for answers. Just as you shouldn'tfly a plane until multiple systems are checked and rechecked, we shouldn't relyon polls alone to tell us how we're doing. Indeed, there were signs this yearthat the Democrats' electoral map was more fragile than the polls made it appearto be.While the polls gave conflicting signals about the state of play in the upperMidwest - showing Iowa and Ohio leaning toward Trump, and Michigan and Wisconsintoward Clinton - demographically, these states are not very different, eachhaving a comparatively high share of white voters without college degrees.Demographic modeling by the likes of David Wasserman of the Cook PoliticalReport provided a more accurate view, showing the Rust Belt poised to movesolidly toward Trump. Nationally,  our own demographic model, relying on factorssuch as the percentage of white voters without college degrees, outperformedstate polling averages in anticipating which states Trump had a chance offlipping on his way to an electoral college upset.We can learn from the digital world, too. One shortcoming of polls is that theycan be slow to reflect reactions to fast-moving events, particularly in thefinal days before an election. Using real-time digital data to derive theseinsights is intriguing, yet top-line figures such as numbers of searches ormentions are rarely illuminating. In fact, our own analysis of publiclyavailable Google trends found that these measures tended to be negativelycorrelated with candidate performance. In the 30 days before the election,"Donald Trump" was Googled more in states that he lost than in those he won.It's digging to understand exactly who is talking about the candidates wherethings get interesting. Throughout the election, we tracked conversations amongthousands of partisans on Twitter to gauge which side seemed more interested andenthusiastic at any given moment. The data in the last few weeks was revealing:The volume of liberal attacks on Trump began to decline around the time of thefinal debate, while conservatives continued to ratchet up the pressure onClinton heading into Election Day. This reflected a dynamic in which Clinton wasfending off a potentially rekindled FBI investigation, while Trump was behavingmore like a normal Republican nominee. Was flagging liberal interest inattacking Trump a possible indicator that Democrats might not surge to the pollsthe way they did in 2008 and 2012? While we should generally be skeptical ofenthusiasm metrics - from yard signs to crowd sizes - the ability to rigorouslyquantify partisan energy levels in online conversations could help answer thatelusive question: Who is likely to vote?Positive online energy, too, may have been an indicator. Media consultant ErinPettigrew examined Facebook interest in the candidates, finding that it beatpolling in predicting the outcome in some states, including Pennsylvania andMichigan. Overall, this Facebook data did not outperform all the state polls,but when combined with the polls, we found that it gave a more accurate forecastof the electoral college than the polls on their own.Electoral surprises around the globe - from Trump to Brexit - show that we maybe overly reliant on horse-race polling. But polls are useful for what they tellus beyond who will win the election. Many of the attitudes Trump rallied votersaround (discontent with the status quo and mistrust of political and economicelites )  were popular in polls, even when he wasn't. This may have been aharbinger of his success and a clue as to where voters would eventuallygravitate. The growing availability of data, from digital and traditionalsources, and the ability to analyze it mean we now have more than just polls tounderstand the electorate.Twitter: @KSoltisAnderson                                 @PatrickRuffini